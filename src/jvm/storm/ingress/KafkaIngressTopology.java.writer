package storm.ingress;

import backtype.storm.Config;
import backtype.storm.LocalCluster;
import backtype.storm.LocalDRPC;
import backtype.storm.StormSubmitter;
import backtype.storm.generated.StormTopology;
import backtype.storm.spout.SchemeAsMultiScheme;
import backtype.storm.task.IMetricsContext;
import backtype.storm.task.TopologyContext;
import backtype.storm.tuple.Fields;
import backtype.storm.tuple.Values;
import clojure.lang.Numbers;
import com.google.common.collect.Lists;
import com.google.common.hash.HashFunction;
import com.google.common.hash.Hashing;
import org.json.JSONException;
import org.json.JSONObject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import storm.ingress.kafka.BrokerHosts;
import storm.ingress.kafka.StringScheme;
import storm.ingress.kafka.ZkHosts;
import storm.ingress.kafka.trident.OpaqueTridentKafkaSpout;
import storm.ingress.kafka.trident.TridentKafkaConfig;
import storm.ingress.postgresql.PostgresqlState;
import storm.ingress.postgresql.PostgresqlStateConfig;
import storm.ingress.postgresql.postgresConnector;
import storm.trident.TridentState;
import storm.trident.TridentTopology;
import storm.trident.fluent.GroupedStream;
import storm.trident.Stream;
import storm.trident.operation.BaseFilter;
import storm.trident.operation.BaseFunction;
import storm.trident.operation.CombinerAggregator;
import storm.trident.operation.ReducerAggregator;
import storm.trident.operation.TridentCollector;
import storm.trident.operation.builtin.Count;
import storm.trident.operation.builtin.MapGet;
import storm.trident.operation.builtin.Sum;
import storm.trident.spout.IBatchSpout;
import storm.trident.state.ReadOnlyState;
import storm.trident.state.State;
import storm.trident.state.StateFactory;
import storm.trident.state.StateType;
import storm.trident.state.map.ReadOnlyMapState;
import storm.trident.tuple.TridentTuple;

import java.io.IOException;
import java.util.*;
import java.util.Map.Entry;
import java.sql.*;


public class KafkaIngressTopology {
         protected KafkaIngressTopology() // prevents calls from subclass
         {
                         throw new UnsupportedOperationException();
         }


         public static final Logger LOG = LoggerFactory.getLogger(KafkaIngressTopology.class);


           /**   Print message published by Kafka.  **/
	    @SuppressWarnings("serial")
		public static class PrintStream extends BaseFunction
        {

	   		@SuppressWarnings("rawtypes")

            @Override
            public void execute(TridentTuple tuple, TridentCollector collector)
            {
//                    String val = tuple.getString(0);
                    System.out.println(tuple.getString(0) + " , " + tuple.getString(1));
            }
	 }

        @SuppressWarnings("serial")
        static class RandomTupleSpout implements IBatchSpout {
            private transient Random random;
            private static final int BATCH = 1000;

            @Override
            @SuppressWarnings("rawtypes")
            public void open(final Map conf, final TopologyContext context) {
            random = new Random();
        }

            @Override
            public void emitBatch(final long batchId, final TridentCollector collector) {
                // emit a 3 number tuple (a,b,c)
                  for (int i = 0; i < BATCH; i++) {
//                        collector.emit(new Values(random.nextInt(1000) + 1, random.nextInt(100) + 1, random.nextInt(100) + 1));
                          collector.emit(new Values(i, "test string inserted into this table"));
                  }
            }

            @Override
            public void ack(final long batchId) {}

            @Override
            public void close() {}

            @Override
            @SuppressWarnings("rawtypes")
            public Map getComponentConfiguration() {
                   return null;
            }

            @Override
            public Fields getOutputFields() {
                 return new Fields("batchid", "word");
    //               return new Fields("a","b","c");
            }
        }


          /**  Parses JSON published by Kafka.   **/
        @SuppressWarnings("serial")
        public static class JsonObjectParse extends BaseFunction {

                   @Override
                   public final void execute(final TridentTuple tuple, final TridentCollector collector)
                   {
                       String val = tuple.getString(0);
    //                   String userid;
                       int userid;
                       JSONObject json = new JSONObject(val);
                       try {
    //                         userid = Integer.toString(json.getInt("userid") )  ;
                               userid = json.getInt("userid") ;
//                               userid = tuple.getInteger(0) ;
                       }
                       catch (JSONException e) {
                    //         System.err.println("Caught JSONException: " + e.getMessage());
                    //         userid = "anonymous";
                               userid = -1 ;
                       }

                       collector.emit(new Values(userid, val));
                   }
        }



        @SuppressWarnings("serial")
        static class ThroughputLoggingFilter extends BaseFilter {

            private static final org.apache.log4j.Logger logger = org.apache.log4j.Logger.getLogger(ThroughputLoggingFilter.class);
            private long count = 0;
            private Long start = System.nanoTime();
            private Long last = System.nanoTime();

            public boolean isKeep(final TridentTuple tuple) {
                 count += 1;
                 final long now = System.nanoTime();
                 if (now - last > 5000000000L) { // emit every 5 seconds
                      logger.info("tuples per second = " + (count * 1000000000L) / (now - start));
                      last = now;
                 }
                 return true;
            }
        }

        @SuppressWarnings("serial")
        static class CountSumSum implements CombinerAggregator<List<Number>> {
//         static class CountSumSum implements CombinerAggregator<List<Number>> {
            @Override
            public List<Number> init(TridentTuple tuple) {
                     System.out.println("b = " + tuple.getValue(0));
                     System.out.println("c = " + tuple.getValue(1));
                     return Lists.newArrayList(1L, (Number) tuple.getValue(0), (Number) tuple.getValue(1));
            }

            @Override
            public List<Number> combine(List<Number> val1, List<Number> val2) {
//                        System.out.println("val1 = " + val1);
//                       System.out.println("val2 = " + val2);
                       return Lists.newArrayList(Numbers.add(val1.get(0), val2.get(0)), Numbers.add(val1.get(1), val2.get(1)), Numbers.add(val1.get(2), val2.get(2)));
            }

            @Override
            public List<Number> zero() {
            return Lists.newArrayList((Number) 0, (Number) 0, (Number) 0);
            }
        }


        @SuppressWarnings("serial")
        static class EventUpdater implements ReducerAggregator<List<String>> {

            @Override
            public List<String> init(){
                     return null;
            }

            @Override
            public List<String> reduce(List<String> curr, TridentTuple tuple) {
                   List<String> updated = null ;

                   if ( curr == null ) {
//                                  Number a = (Number) tuple.getValue(0);
//                                  Number b = (Number) tuple.getValue(1);
//                                  Number c = (Number) tuple.getValue(2);
//                                  updated = Lists.newArrayList(a, b, c);
                                    String event = (String) tuple.getValue(1);
                                    updated = Lists.newArrayList(event);

                   } else {
                                    updated = curr ;
                   }
//              System.out.println(updated);
              return updated ;
            }
        }


            /**  Storm topology   **/
        public static StormTopology buildTridentKafkaTopology() throws IOException {
                String dburl = "jdbc:postgresql://10.100.70.23:5432/ingest" ;
//                postgresConnector connector = new postgresConnector() ;
//                connector.connectIngress();
                BrokerHosts zk = new ZkHosts("10.100.70.128:2181");
	       		TridentKafkaConfig spoutConf = new TridentKafkaConfig(zk, "topictest");
        		spoutConf.scheme = new SchemeAsMultiScheme(new StringScheme());
                spoutConf.fetchSizeBytes = 50*1024*1024;
                spoutConf.bufferSizeBytes = 50*1024*1024;
	       		OpaqueTridentKafkaSpout kafkaSpout = new OpaqueTridentKafkaSpout(spoutConf);
//                TransactionalTridentKafkaSpout kafkaSpout = new TransactionalTridentKafkaSpout(spoutConf);
                TridentTopology topology = new TridentTopology();


//                final GroupedStream stream = topology.newStream("test", new RandomTupleSpout())
//                                                .each(new Fields(), new ThroughputLoggingFilter());
//                                               .each(new Fields("word"),
//                                                     new PrintStream(),
//                                                     new Fields("word1"));
//                                                 .groupBy(new Fields("batchid"));


                final PostgresqlStateConfig config = new PostgresqlStateConfig();
                {
                      config.setUrl(dburl);
                      config.setTable("test.state");
                      config.setKeyColumns(new String[]{"userid"});
          //            config.setValueColumns(new String[]{"a1","b","c"});
                      config.setValueColumns(new String[]{"event"});
                      config.setType(StateType.NON_TRANSACTIONAL);
                      config.setCacheSize(5000);
                }


                topology.newStream("topictestspout", kafkaSpout)
//                  topology.newStream("test", new RandomTupleSpout())        // this test tells the kafkaSpout has the overhead to cause the latency
                                      .parallelismHint(4)
//                                      .shuffle()
//                                      .each(new Fields("batchid","word"),
                                      .each(new Fields("str"),
                                            new JsonObjectParse(),
                                            new Fields("userid","event"))
                                      .groupBy(new Fields("userid"))
                                      .persistentAggregate(PostgresqlState.newFactory(config), new Fields("userid","event"), new EventUpdater(), new Fields( "eventword"));
//                                      .parallelismHint(32);
//                stream.persistentAggregate(PostgresqlState.newFactory(config), new Fields("b","c"), new CountSumSum(), new Fields("sum"));

	            return topology.build();

        }



        public static void main(String[] args) throws Exception {

         	 	  Config conf = new Config();
	         	  conf.setMaxSpoutPending(10);
	        	  conf.setMaxTaskParallelism(3);
	        	  LocalDRPC drpc = new LocalDRPC();
	         	  final LocalCluster cluster = new LocalCluster();

//                  postgresConnector connector = new postgresConnector();
//                  connector.connectIngress();

                  if (args != null && args.length > 0) {
                          cluster.submitTopology("topictest", conf, buildTridentKafkaTopology());
//                          while (true) {}

//                          Thread.sleep(1500);
//
                      //cluster.shutdown();
                      //drpc.shutdown();
                  }
                  else {
                          conf.setNumWorkers(10);
                          StormSubmitter.submitTopology(args[0], conf, buildTridentKafkaTopology());
                  }

	 	}


}
