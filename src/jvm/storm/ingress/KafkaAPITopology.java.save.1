package storm.ingress;

import backtype.storm.Config;
import backtype.storm.LocalCluster;
import backtype.storm.StormSubmitter;
import backtype.storm.spout.SchemeAsMultiScheme;
import backtype.storm.task.TopologyContext;
import backtype.storm.tuple.Fields;
import backtype.storm.tuple.Values;
import com.google.common.collect.Lists;
import org.json.JSONException;
import org.json.JSONObject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import storm.ingress.kafka.Broker;
import storm.ingress.kafka.BrokerHosts;
import storm.ingress.kafka.StringScheme;
import storm.ingress.kafka.ZkHosts;
import storm.ingress.kafka.trident.GlobalPartitionInformation;
import storm.ingress.kafka.trident.OpaqueTridentKafkaSpout;
import storm.ingress.kafka.trident.TridentKafkaConfig;
import storm.ingress.postgresql.PostgresqlStateConfig;
import storm.ingress.postgresql.PostgresqlState;
import storm.trident.TridentTopology;
import storm.trident.operation.BaseFunction;
import storm.trident.operation.ReducerAggregator;
import storm.trident.operation.TridentCollector;
import storm.trident.spout.IBatchSpout;
import storm.trident.state.StateType;
import storm.trident.tuple.TridentTuple;

import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStream;
import java.util.*;
import java.util.regex.Pattern;
import java.util.regex.Matcher;
import java.util.Date;


public class KafkaAPITopology {
    protected KafkaAPITopology() // prevents calls from subclass
    {
        throw new UnsupportedOperationException();
    }


    public static final Logger LOG = LoggerFactory.getLogger(KafkaAPITopology.class);


    /**
     * Print message published by Kafka.  *
     */
    @SuppressWarnings("serial")
    public static class PrintStream extends BaseFunction {

        @SuppressWarnings("rawtypes")

        @Override
        public void execute(TridentTuple tuple, TridentCollector collector) {
            System.out.println("***:" + tuple + ":***");
        }
    }

    @SuppressWarnings("serial")
    static class RandomTupleSpout implements IBatchSpout {
        private transient Random random;
        private static final int BATCH = 40;

        @Override
        @SuppressWarnings("rawtypes")
        public void open(final Map conf, final TopologyContext context) {
            random = new Random();
        }

        @Override
        public void emitBatch(final long batchId, final TridentCollector collector) {
            for (int i = 0; i < BATCH; i++) {
                collector.emit(new Values("test string inserted into this table " + i));

            }
        }

        @Override
        public void ack(final long batchId) {
        }

        @Override
        public void close() {
        }

        @Override
        @SuppressWarnings("rawtypes")
        public Map getComponentConfiguration() {
            return null;
        }

        @Override
        public Fields getOutputFields() {
            //      return new Fields("userid", "event");
            return new Fields("event");
        }
    }


    /**
     * Parses JSON published by Kafka.   *
     */
    @SuppressWarnings("serial")
    public static class JsonObjectParse extends BaseFunction {
        Pattern p = Pattern.compile("([0-9]+)");

        @Override
        public final void execute(final TridentTuple tuple, final TridentCollector collector) {
            String event = tuple.getString(0);
//            System.out.println("+++===" + tuple + "===+++");
//            System.out.println("+++===" + collector + "===+++");

            int userId = -1;
            String appSessionId = null;
            int eventId = -1;
            String eventType = null;
            Date eventTime = null;

            try{
                JSONObject json = new JSONObject(event);
            
                userId = json.getInt("userId");
                appSessionId = json.getString("appSessionId");
                eventId = json.getInt("eventId");
                eventType = json.getString("eventType");
                String eventTimeStr = json.getString("timestamp");
                //  this string is a UTC date string embedded in a "/Date(nnnnn)/"
                // use a regex to parse out the date
                Matcher m = p.matcher(eventTimeStr);
                if (m.matches()) {
                    long eventTimeStamp = Long.getLong(m.group(1));
                    eventTime = new Date(eventTimeStamp * 1000);
                }
            } catch (JSONException e) {
                // we have an error, need to do something to raise this up the chain
//                System.out.println("ERROR: " + e.getMessage());
            }

            collector.emit(new Values(userId, appSessionId, eventId, eventType, eventTime, event));
        }
    }


    @SuppressWarnings("serial")
    static class EventUpdater implements ReducerAggregator<List<String>> {

        @Override
        public List<String> init() {
            return null;
        }

        @Override
        public List<String> reduce(List<String> curr, TridentTuple tuple) {
            // expecting the appsessionId (0) and the eventid (1)
            // data is in the 
//            System.out.println("===reduce===");
            List<String> updated = null;

/*
            try{
//                System.out.println("reducer: " + tuple.getString(0) + ", " + tuple.getInteger(1) + ", " + tuple.getString(2)); 
            } catch (Exception e) {
                System.out.println("ERROR: reducer" + e.getMessage() + " [" + tuple + "]");
            }
*/
            if (curr == null) {
                String event = (String) tuple.getValue(2);
                updated = Lists.newArrayList(event);

            } else {
                updated = curr;
            }
            System.out.println(updated);
            return updated;
        }
    }


    public static List<String> CrunchifyGetPropertyValues() {
        Properties prop = new Properties();
        String propFileName = "./config/ingress.config";
        try {
            InputStream inputStream = new FileInputStream(propFileName);
            prop.load(inputStream);
        } catch (FileNotFoundException e) {
            e.printStackTrace();
        } catch (IOException e) {
            e.printStackTrace();
        }
        String durl = prop.getProperty("db.url");
        String brokerip = prop.getProperty("broker.ip");
        String zkhosts = prop.getProperty("zk.host");
        String topicname = prop.getProperty("topic.name");
        List<String> configs = new ArrayList<String>();
        configs.add(durl);
        configs.add(brokerip);
        configs.add(zkhosts);
        configs.add(topicname);
        return configs;
    }


    public static void main(String[] args) throws Exception {

        String dburl = CrunchifyGetPropertyValues().get(0);
        Broker broker = Broker.fromString(CrunchifyGetPropertyValues().get(1));
        GlobalPartitionInformation info = new GlobalPartitionInformation();
//                 int partitionCount = 8;
//                 for(int i =0;i<partitionCount;i++){
//                                info.addPartition(i, broker) ;
//                 }

//                 StaticHosts hosts = new StaticHosts(info);
//                 StaticHosts kafkaHosts = KafkaConfig.StaticHosts.fromHostString(Arrays.asList(new String[] { "localhost" }), 1);
        BrokerHosts zk = new ZkHosts(CrunchifyGetPropertyValues().get(2));
        System.out.println("topic=>" + CrunchifyGetPropertyValues().get(3));
        TridentKafkaConfig spoutConf = new TridentKafkaConfig(zk, CrunchifyGetPropertyValues().get(3));
//                 TridentKafkaConfig spoutConf = new TridentKafkaConfig(kafkaHosts, CrunchifyGetPropertyValues().get(3));
        spoutConf.scheme = new SchemeAsMultiScheme(new StringScheme());

//                 spoutConf.fetchSizeBytes = 50*1024*1024;
//                 spoutConf.bufferSizeBytes = 50*1024*1024;
        spoutConf.startOffsetTime = kafka.api.OffsetRequest.EarliestTime();
//                spoutConf.startOffsetTime = kafka.api.OffsetRequest.LatestTime();
//                 spoutConf.socketTimeoutMs = 1000;
//                 spoutConf.fetchMaxWait = 1000;
        spoutConf.forceFromStart = true;
//                 spoutConf.maxOffsetBehind = Long.MAX_VALUE;
//                 spoutConf.useStartOffsetTimeIfOffsetOutOfRange = true;
//                 spoutConf.metricsTimeBucketSizeInSecs = 600;

        OpaqueTridentKafkaSpout kafkaSpout = new OpaqueTridentKafkaSpout(spoutConf);
//                TransactionalTridentKafkaSpout kafkaSpout = new TransactionalTridentKafkaSpout(spoutConf);
        TridentTopology topology = new TridentTopology();


        final PostgresqlStateConfig config = new PostgresqlStateConfig();
        {
            config.setUrl(dburl);
            
            config.setTable("input.events_storm_test"); //"input.events_session_test");
            config.setKeyColumns(new String[]{"event_time","event_type"}); //new String[]{"appSessionId", "eventId"});
            config.setValueColumns(new String[]{"event_object"});
    
            config.setType(StateType.NON_TRANSACTIONAL);
            config.setCacheSize(5000);
        }

        topology.newStream("spoutInit", kafkaSpout)
                //.each(new Fields("str"), new PrintStream())
                .each(new Fields("str"),
                        new JsonObjectParse(),
                        new Fields("userId", "appSessionId", "eventId", "eventType", "eventTime", "event"))
                //.parallelismHint(6)
                .groupBy(new Fields("appSessionId", "eventId"))
                //.aggregate(
                //.each()
                //.persistentAggregate(PostgresqlState.newFactory(config), new Fields("appSessionId", "eventId", "event"), new EventUpdater(), new Fields("eventword"));
                .persistentAggregate(PostgresqlState.newFactory(config), new Fields("eventTime", "eventType", "event"), new EventUpdater(), new Fields("eventword"));

        Config conf = new Config();
//                  conf.setDebug(true);

        if (args != null && args.length > 0) {
            System.out.println("Storm cluster....");
            conf.setNumWorkers(10);
//                          StormSubmitter.submitTopology(args[0], conf, buildTridentKafkaTopology());
            StormSubmitter.submitTopology(args[0], conf, topology.build());
        } else {
            System.out.println("local mode....");
            //   conf.setMaxSpoutPending(10);
            conf.setMaxTaskParallelism(10);
//                          LocalDRPC drpc = new LocalDRPC();
            LocalCluster cluster = new LocalCluster();


            cluster.submitTopology("apiTop", conf, topology.build());
//                          while (true) {}

//                          Thread.sleep(1500);
//
//                          cluster.shutdown();
//                          drpc.shutdown();

        }

    }


}

